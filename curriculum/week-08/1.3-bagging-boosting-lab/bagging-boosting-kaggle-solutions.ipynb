{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "\n",
    "## Bagging and Boosting vs. Regression\n",
    "\n",
    "Week 8 | 1.3\n",
    "\n",
    "---\n",
    "\n",
    "This lab uses the housing data from Project 3 to compare bagging and boosting ensemble methods to regression.\n",
    "\n",
    "### 1. Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You may want to load a cleaned/feature engineered version of the data that you have from project 3 rather than the raw housing data file (to skip the data munging and cleaning part.**\n",
    "\n",
    "My path to the raw file is below; replace this with your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "house = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-4/datasets/housing_regression/housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed = pd.read_csv('./train_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81) (2919, 146)\n"
     ]
    }
   ],
   "source": [
    "print house.shape, processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Intercept', u'BldgType[T.2fmCon]', u'BldgType[T.Duplex]',\n",
      "       u'BldgType[T.Twnhs]', u'BldgType[T.TwnhsE]', u'CentralAir[T.Y]',\n",
      "       u'ConditionOne[T.Feedr]', u'ConditionOne[T.Norm]',\n",
      "       u'ConditionOne[T.PosA]', u'ConditionOne[T.PosN]',\n",
      "       ...\n",
      "       u'OverallQual', u'PoolArea', u'ScreenPorch', u'TotRmsAbvGrd',\n",
      "       u'WoodDeckSF', u'YearBuilt', u'YearRemodAdd', u'YrAgeWhenSold',\n",
      "       u'YrSold', u'is_test'],\n",
      "      dtype='object', length=146)\n"
     ]
    }
   ],
   "source": [
    "print processed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1459 2919\n"
     ]
    }
   ],
   "source": [
    "print processed['is_test'].sum(), processed['is_test'].sum()+house.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Id', u'MSSubClass', u'MSZoning', u'LotFrontage', u'LotArea',\n",
       "       u'Street', u'Alley', u'LotShape', u'LandContour', u'Utilities',\n",
       "       u'LotConfig', u'LandSlope', u'Neighborhood', u'Condition1',\n",
       "       u'Condition2', u'BldgType', u'HouseStyle', u'OverallQual',\n",
       "       u'OverallCond', u'YearBuilt', u'YearRemodAdd', u'RoofStyle',\n",
       "       u'RoofMatl', u'Exterior1st', u'Exterior2nd', u'MasVnrType',\n",
       "       u'MasVnrArea', u'ExterQual', u'ExterCond', u'Foundation', u'BsmtQual',\n",
       "       u'BsmtCond', u'BsmtExposure', u'BsmtFinType1', u'BsmtFinSF1',\n",
       "       u'BsmtFinType2', u'BsmtFinSF2', u'BsmtUnfSF', u'TotalBsmtSF',\n",
       "       u'Heating', u'HeatingQC', u'CentralAir', u'Electrical', u'1stFlrSF',\n",
       "       u'2ndFlrSF', u'LowQualFinSF', u'GrLivArea', u'BsmtFullBath',\n",
       "       u'BsmtHalfBath', u'FullBath', u'HalfBath', u'BedroomAbvGr',\n",
       "       u'KitchenAbvGr', u'KitchenQual', u'TotRmsAbvGrd', u'Functional',\n",
       "       u'Fireplaces', u'FireplaceQu', u'GarageType', u'GarageYrBlt',\n",
       "       u'GarageFinish', u'GarageCars', u'GarageArea', u'GarageQual',\n",
       "       u'GarageCond', u'PavedDrive', u'WoodDeckSF', u'OpenPorchSF',\n",
       "       u'EnclosedPorch', u'3SsnPorch', u'ScreenPorch', u'PoolArea', u'PoolQC',\n",
       "       u'Fence', u'MiscFeature', u'MiscVal', u'MoSold', u'YrSold', u'SaleType',\n",
       "       u'SaleCondition', u'SalePrice'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = house['SalePrice'].values\n",
    "ln_output = np.log(output+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 146) (1459, 146)\n"
     ]
    }
   ],
   "source": [
    "train, test = processed[processed['is_test'] == 0], processed[processed['is_test'] == 1]\n",
    "print train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "train.drop('Intercept', axis=1, inplace=True)\n",
    "test.drop('Intercept', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 145) (1459, 145)\n"
     ]
    }
   ],
   "source": [
    "print train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Build a decision tree regressor\n",
    "\n",
    "1. Train a decision tree regressor on the regression problem (predicting `SalePrice` or a transformed version of it from predictors of your choice.)\n",
    "- Evaluate the score with a 5-fold cross-validation\n",
    "- How does this compare to the model you fit on this data for Project 3?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = ln_output\n",
    "X = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 168 candidates, totalling 840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 840 out of 840 | elapsed:    7.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best'),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'min_samples_split': [2, 4, 8, 16, 32, 64, 128], 'max_features': [None, 'sqrt', 'log2'], 'max_depth': [2, 3, 4, 5, 6, 7, 8, None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = DecisionTreeRegressor()\n",
    "dtc_params = {\n",
    "    'max_depth':[2,3,4,5,6,7,8,None],\n",
    "    'min_samples_split':[2,4,8,16,32,64,128],\n",
    "    'max_features':[None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "dtc_gs = GridSearchCV(dtc, dtc_params, cv=5, verbose=1)\n",
    "dtc_gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': None, 'min_samples_split': 32, 'max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "print dtc_gs.best_params_\n",
    "dtc_best = dtc_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.73845805  0.73312997  0.79097556  0.76427937  0.72259966]\n",
      "0.749888523227 0.0247039596558\n"
     ]
    }
   ],
   "source": [
    "dtc_scores = cross_val_score(DecisionTreeRegressor(**dtc_gs.best_params_), X, y, cv=5)\n",
    "print dtc_scores\n",
    "print np.mean(dtc_scores), np.std(dtc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Build a random forest regressor\n",
    "\n",
    "1. Train a random forest regressor on the regression problem.\n",
    "- Evaluate the score with a 5-fold cross-validation\n",
    "- How does this compare to the models you fit on this data previously?\n",
    "\n",
    "You may want to use a gridsearch to find the optimal parameters. Be careful to not put too many different options/parameters into the gridsearch or it will take a long time to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 126 candidates, totalling 630 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 630 out of 630 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [200], 'min_samples_split': [2, 4, 8, 16, 32, 64, 128], 'max_depth': [2, 3, 4, 5, 6, None], 'max_features': [None, 'sqrt', 'log2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators':[200],\n",
    "    'max_depth':[2,3,4,5,6,None],\n",
    "    'min_samples_split':[2,4,8,16,32,64,128],\n",
    "    'max_features':[None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_gs = GridSearchCV(rf, rf_params, cv=5, verbose=1, n_jobs=-1)\n",
    "rf_gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200, 'max_depth': None}\n",
      "[ 0.8722039   0.8578691   0.85176007  0.86764115  0.84539344]\n",
      "0.858973531787 0.00987768151303\n"
     ]
    }
   ],
   "source": [
    "print rf_gs.best_params_\n",
    "rf_best = rf_gs.best_estimator_\n",
    "\n",
    "rf_scores = cross_val_score(rf_best, X, y, cv=5)\n",
    "print rf_scores\n",
    "print np.mean(rf_scores), np.std(rf_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Build an AdaBoost regressor and/or a gradient boosted regressor\n",
    "\n",
    "The models both allow you to change the base estimator (they default to decision tree regressors, which I would recommend). The most important parameters in Adaboost besides the `base_estimator` for each model are:\n",
    "\n",
    "    n_estimators: how many weak learners to chain together\n",
    "    learning_rate: how much should the contribution of subsequent weak learners be \"shrunk\" (this means that in addition to the reweighting, subsequent weak learners are also forced to have a smaller impact.)\n",
    "    \n",
    "The gradient boosting regressor forces you to use decision trees, but allows you to modify components of each weak learner tree through its keyword arguments, such as `max_depth`, `max_features`, `min_samples_split`, etc. \n",
    "\n",
    "1. Build the model(s). You may want to find the best parameters with a gridsearch.\n",
    "2. Evaluate the score using cross-validation as before.\n",
    "3. How does boosting compare to bagging (random forest) and the original model you built for your project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:   51.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
       "         n_estimators=50, random_state=None),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [25, 50, 100, 150, 200], 'learning_rate': [0.5, 0.1, 0.05, 0.01, 0.001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = AdaBoostRegressor()\n",
    "\n",
    "ada_params = {\n",
    "    'n_estimators':[25,50,100,150,200],\n",
    "    'learning_rate':[0.5,0.1,0.05,0.01,0.001],\n",
    "}\n",
    "\n",
    "ada_gs = GridSearchCV(ada, ada_params, cv=5, verbose=1, n_jobs=-1)\n",
    "ada_gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 150, 'learning_rate': 0.5}\n",
      "[ 0.75730924  0.82841149  0.7944712   0.7745997   0.7750997 ]\n",
      "0.785978265801 0.0242587567211\n"
     ]
    }
   ],
   "source": [
    "print ada_gs.best_params_\n",
    "ada_best = ada_gs.best_estimator_\n",
    "\n",
    "ada_scores = cross_val_score(ada_best, X, y, cv=5)\n",
    "print ada_scores\n",
    "print np.mean(ada_scores), np.std(ada_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1728 candidates, totalling 8640 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   23.9s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4992 tasks      | elapsed: 24.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6042 tasks      | elapsed: 31.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7192 tasks      | elapsed: 39.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8442 tasks      | elapsed: 48.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8640 out of 8640 | elapsed: 53.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'loss': ['ls', 'huber'], 'subsample': [1.0, 0.75, 0.5], 'learning_rate': [0.5, 0.25, 0.1, 0.01], 'n_estimators': [200], 'max_features': [None, 'sqrt', 'log2'], 'min_samples_split': [2, 16, 64, 128], 'max_depth': [2, 3, 4, 5, 6, None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "gb_params = {\n",
    "    'n_estimators':[200],\n",
    "    'loss':['ls','huber'],\n",
    "    'learning_rate':[0.5,0.25,0.1,0.01],\n",
    "    'max_depth':[2,3,4,5,6,None],\n",
    "    'min_samples_split':[2,16,64,128],\n",
    "    'max_features':[None, 'sqrt', 'log2'],\n",
    "    'subsample':[1.0,0.75,0.5]\n",
    "}\n",
    "\n",
    "gb_gs = GridSearchCV(gb, gb_params, cv=5, verbose=1, n_jobs=-1)\n",
    "gb_gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 'huber', 'subsample': 0.5, 'learning_rate': 0.1, 'n_estimators': 200, 'max_features': None, 'min_samples_split': 2, 'max_depth': 3}\n",
      "[ 0.89694798  0.8849864   0.87471605  0.8874101   0.86502796]\n",
      "0.881817698945 0.0109778850747\n"
     ]
    }
   ],
   "source": [
    "print gb_gs.best_params_\n",
    "gb_best = gb_gs.best_estimator_\n",
    "\n",
    "gb_scores = cross_val_score(gb_best, X, y, cv=5)\n",
    "print gb_scores\n",
    "print np.mean(gb_scores), np.std(gb_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. [Bonus] Submit to kaggle with your bagging and/or boosting model.\n",
    "\n",
    "Using the test data like we did in class, make predictions and submit your score to kaggle. Does your ensemble model perform better?\n",
    "\n",
    "I've put the path to my test data below (which should be in the equivalent folder in your repo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-4/datasets/housing_regression/test_houses.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
