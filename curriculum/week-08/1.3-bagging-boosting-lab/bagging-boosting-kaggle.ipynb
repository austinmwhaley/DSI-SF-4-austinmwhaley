{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "\n",
    "## Bagging and Boosting vs. Regression\n",
    "\n",
    "Week 8 | 1.3\n",
    "\n",
    "---\n",
    "\n",
    "This lab uses the housing data from Project 3 to compare bagging and boosting ensemble methods to regression.\n",
    "\n",
    "### 1. Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You may want to load a cleaned/feature engineered version of the data that you have from project 3 rather than the raw housing data file (to skip the data munging and cleaning part.**\n",
    "\n",
    "My path to the raw file is below; replace this with your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "house = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-4/datasets/housing_regression/housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Build a decision tree regressor\n",
    "\n",
    "1. Train a decision tree regressor on the regression problem (predicting `SalePrice` or a transformed version of it from predictors of your choice.)\n",
    "- Evaluate the score with a 5-fold cross-validation\n",
    "- How does this compare to the model you fit on this data for Project 3?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Build a random forest regressor\n",
    "\n",
    "1. Train a random forest regressor on the regression problem.\n",
    "- Evaluate the score with a 5-fold cross-validation\n",
    "- How does this compare to the models you fit on this data previously?\n",
    "\n",
    "You may want to use a gridsearch to find the optimal parameters. Be careful to not put too many different options/parameters into the gridsearch or it will take a long time to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Build an AdaBoost regressor and/or a gradient boosted regressor\n",
    "\n",
    "The models both allow you to change the base estimator (they default to decision tree regressors, which I would recommend). The most important parameters in Adaboost besides the `base_estimator` for each model are:\n",
    "\n",
    "    n_estimators: how many weak learners to chain together\n",
    "    learning_rate: how much should the contribution of subsequent weak learners be \"shrunk\" (this means that in addition to the reweighting, subsequent weak learners are also forced to have a smaller impact.)\n",
    "    \n",
    "The gradient boosting regressor forces you to use decision trees, but allows you to modify components of each weak learner tree through its keyword arguments, such as `max_depth`, `max_features`, `min_samples_split`, etc. \n",
    "\n",
    "1. Build the model(s). You may want to find the best parameters with a gridsearch.\n",
    "2. Evaluate the score using cross-validation as before.\n",
    "3. How does boosting compare to bagging (random forest) and the original model you built for your project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. [Bonus] Submit to kaggle with your bagging and/or boosting model.\n",
    "\n",
    "Using the test data like we did in class, make predictions and submit your score to kaggle. Does your ensemble model perform better?\n",
    "\n",
    "I've put the path to my test data below (which should be in the equivalent folder in your repo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-4/datasets/housing_regression/test_houses.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
