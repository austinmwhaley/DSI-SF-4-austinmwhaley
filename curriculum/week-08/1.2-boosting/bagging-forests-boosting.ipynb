{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "\n",
    "## Boosting\n",
    "\n",
    "Week 8 | 1.2\n",
    "\n",
    "---\n",
    "\n",
    "Boosting is another ensemble method with a theoretically different approach than bagging.\n",
    "\n",
    "\n",
    "1. **Base model fitting is an iterative procedure**: it cannot be run in parallel.\n",
    "- **Weights assigned to observations indicating their \"importance\"**: samples with higher weights are given higher influence on the total error of the next model, prioritizing those observations.\n",
    "- **Weights change at each iteration with the goal of correcting the errors/misclassifications of the previous iteration**: the first base estimator is fit with uniform weights on the observations.\n",
    "- **Final prediction is typically constructed by a weighted vote**: weights for each base model depends on their training errors or misclassification rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pros and cons\n",
    "\n",
    "---\n",
    "\n",
    "**PROS**\n",
    "\n",
    "- Achieves higher performance than bagging when hyper-parameters tuned properly.\n",
    "- Can be used for classification and regression equally well.\n",
    "- Easily handles mixed data types.\n",
    "- Can use \"robust\" loss functions that make the model resistant to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "**CONS**\n",
    "\n",
    "- Difficult and time consuming to properly tune hyper-parameters.\n",
    "- Cannot be parallelized like bagging (bad scalability when huge amounts of data).\n",
    "- More risk of overfitting compared to bagging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![boostvsbag](./images/BoostingVSBagging.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Boosting and bias-variance \n",
    "\n",
    "---\n",
    "\n",
    "Recall that **bagging aims to reduce variance**.\n",
    "\n",
    "**Boosting aims to reduce bias!** (and can reduce variance a bit as well).\n",
    "\n",
    "---\n",
    "\n",
    "#### Why?\n",
    "\n",
    "The rationale/theory behind Boosting is to combines **many weak learners into a single strong learner.**\n",
    "\n",
    "Instead of deep/full decision trees like in bagging, **Boosting uses shallow/high bias base estimators.**\n",
    "\n",
    "Thus, each weak learner has:\n",
    "\n",
    "- Low variance\n",
    "- High bias\n",
    "\n",
    "And the iterative fitting to explain error/misclassification unexplained by the previous base models reduces bias without increasing variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost\n",
    "\n",
    "---\n",
    "\n",
    "AdaBoost is the original boosting algorithm. Predictions from AdaBoost follow the formula:\n",
    "\n",
    "\n",
    "### $$ AdaBoost(X) = sign\\left(\\sum_{t=1}^T\\alpha_t h_t(X)\\right) $$\n",
    "\n",
    "where\n",
    "\n",
    "$AdaBoost(X)$ is the classification predictions for $y$ using predictor matrix $X$\n",
    "\n",
    "$T$ is the set of \"weak learners\"\n",
    "\n",
    "$\\alpha_t$ is the contribution weight for weak learner $t$\n",
    "\n",
    "$h_t(X)$ is the prediction of weak learner $t$\n",
    "\n",
    "and $y$ is binary **with values -1 and 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The weak learner classifiers are trained sequentially.  After each fit, the importance weights on each observation need to be updated. AdaBoost, like all boosting ensemble methods, focuses the next model's fit on the misclassifications/weaknesses of the prior models.\n",
    "\n",
    "All training examples start with equal importance weighting. When we finish training a classifier, we update the importance weighting of the classifier itself represented by alpha $\\alpha$.\n",
    "\n",
    "### $$ \\alpha_t = \\frac{1}{2}ln \\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right) \\text{where } \\epsilon_t < 1$$\n",
    "\n",
    "Where $\\epsilon_t$ is the misclassification rate for the current classifier:\n",
    "\n",
    "### $$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training example weights\n",
    "\n",
    "---\n",
    "\n",
    "Adaboost sets up a weight vector on the observations, denoted $D_t$ where $t$ is the current model iteration. $D_t$ is a probability distribution that determines how likely it is a given observation will be selected as part of the training set for the current estimator.\n",
    "\n",
    "The $\\alpha$ weighting of the last fit estimator is used in the equation for the weighting distribution. The update equation is:\n",
    "\n",
    "### $$ D_{t+1}(i) = D_t(i) e^{-\\alpha_t y_i h_t(x_i)} $$\n",
    "\n",
    "Where $i$ is the vector of observation indices and $x_i$ is the observation at the index. $y_i$ is the target.\n",
    "\n",
    "$h_t$ is the previous model fit in the boosting chain.\n",
    "\n",
    "And then divide the weights by the sum of weights to normalize them, ensuring that they sum to 1 and form a probability distribution:\n",
    "\n",
    "### $$ D_{t+1}(i) = \\frac{D_{t+1}(i)}{\\sum_{i=1}^N D_{t+1}(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Boosting Models\n",
    "\n",
    "---\n",
    "\n",
    "A Gradient Boosting Model (GBM) is a generalization of boosting that is essentially an extension of gradient descent.\n",
    "\n",
    "\n",
    "**In gradient descent, we minimized prediction error with regard to the coefficients $b_1 ... b_i$**\n",
    "\n",
    "**GBM minimizes with respect to the _function_ defining prediction errors $f(x)$**\n",
    "\n",
    "More intuitively, at each step in the GBM:\n",
    "- A model $h(x)$ is constructed to further reduce overall error defined by $f(x)$\n",
    "- The model $h(x)$ therefore _emulates a step descending the gradient of the total error space._ \n",
    "\n",
    "By minimizing the error with respect to the function we can perform the \"gradient descent\" down a loss function like least-squares loss for non-parametric models!\n",
    "\n",
    "---\n",
    "\n",
    "_For more detailed explanations see [here](https://www.quora.com/What-is-an-intuitive-explanation-of-Gradient-Boosting) and [here](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
