{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis codealong using spacy and movie reviews\n",
    "\n",
    "Sentiment analysis is one of the more popular topics in NLP. It is concerned with finding some kind of valence to written text. This could be positivity, negativity, subjectivity and many others. In this lesson we will just be looking at those three. \n",
    "\n",
    "First we will load in a dataset of pre-coded sentiment scores for positivity and negativity on words. These words are also divided up by their part of speech in the sentence.\n",
    "\n",
    "Then we will load snippets of rottentomatoes reviews and explore the sentiment of the writing.\n",
    "\n",
    "---\n",
    "\n",
    "### Load packages and sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sen = pd.read_csv('/Users/austinwhaley/Desktop/DSI-SF-4-austinmwhaley/datasets/sentiment_words/sentiment_words_simple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adj</td>\n",
       "      <td>.22-caliber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adj</td>\n",
       "      <td>.22-calibre</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adj</td>\n",
       "      <td>.22_caliber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adj</td>\n",
       "      <td>.22_calibre</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adj</td>\n",
       "      <td>.38-caliber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos         word  pos_score  neg_score\n",
       "0  adj  .22-caliber        0.0        0.0\n",
       "1  adj  .22-calibre        0.0        0.0\n",
       "2  adj  .22_caliber        0.0        0.0\n",
       "3  adj  .22_calibre        0.0        0.0\n",
       "4  adj  .38-caliber        0.0        0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Create a sentiment dataset that does not take into account part of speech tags\n",
    "\n",
    "This will be what we use first, not knowing the part of speech a word is in. Later when we use spacy we will be able to determine the part of speech of each word and pair the scores accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'hood</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'s_gravenhage</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'tween</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'tween_decks</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.22</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.22-caliber</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.22-calibre</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.22_caliber</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.22_calibre</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.38-caliber</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.38-calibre</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>.38_caliber</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>.38_calibre</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>.45-caliber</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>.45-calibre</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.45_caliber</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>.45_calibre</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1-dodecanol</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  pos_score  neg_score\n",
       "0           'hood      0.000      0.375\n",
       "1   's_gravenhage      0.000      0.000\n",
       "2          'tween      0.000      0.000\n",
       "3    'tween_decks      0.000      0.000\n",
       "4             .22      0.125      0.000\n",
       "5     .22-caliber      0.000      0.000\n",
       "6     .22-calibre      0.000      0.000\n",
       "7     .22_caliber      0.000      0.000\n",
       "8     .22_calibre      0.000      0.000\n",
       "9     .38-caliber      0.000      0.000\n",
       "10    .38-calibre      0.000      0.000\n",
       "11    .38_caliber      0.000      0.000\n",
       "12    .38_calibre      0.000      0.000\n",
       "13    .45-caliber      0.000      0.000\n",
       "14    .45-calibre      0.000      0.000\n",
       "15    .45_caliber      0.000      0.000\n",
       "16    .45_calibre      0.000      0.000\n",
       "17              0      0.000      0.250\n",
       "18              1      0.000      0.125\n",
       "19    1-dodecanol      0.000      0.000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_agg = sen[['word', 'pos_score', 'neg_score']].groupby('word').agg(np.mean).reset_index()\n",
    "sen_agg.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Create a dictionary version of the sentiment data for both the part of speech and aggregate\n",
    "\n",
    "The dictionary format of the data will be much easier to index into in our functions later. If we don't do this it's much harder to make those functions run quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n"
     ]
    }
   ],
   "source": [
    "sen_dict = {\n",
    "    'ADJ':{},\n",
    "    'NOUN':{},\n",
    "    'VERB':{},\n",
    "    'ADV':{}\n",
    "}\n",
    "\n",
    "for i, row in enumerate(sen.itertuples()): #Will go line by line in the dataframe and pull out values in a tuple\n",
    "    if (i % 10000) == 0:\n",
    "        print i\n",
    "    sen_dict[row[1].upper()][row[2]] = {'pos_score': row[3], 'neg_score':row[4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg_score': 0.0, 'pos_score': 0.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_dict['NOUN']['russia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n"
     ]
    }
   ],
   "source": [
    "agg_dict={}\n",
    "\n",
    "for i, row in enumerate(sen_agg.itertuples()): #Will go line by line in the dataframe and pull out values in a tuple\n",
    "    if (i % 10000) == 0:\n",
    "        print i\n",
    "    agg_dict[row[1]] = {'pos_score': row[2], 'neg_score':row[3]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Load the rotten tomatoes dataset\n",
    "\n",
    "This dataset has:\n",
    "    \n",
    "    critic: critic's name\n",
    "    fresh: fresh vs. rotten rating\n",
    "    imdb: code for imdb\n",
    "    publication: where the review was published\n",
    "    quote: the review snippet\n",
    "    review_date: date of review\n",
    "    rtid: rottentomatoes id\n",
    "    title: name of movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt = pd.read_csv('/Users/austinwhaley/Desktop/DSI-SF-4-austinmwhaley/datasets/rottentomatoes_critics/rt_critics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Richard Corliss</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>TIME Magazine</td>\n",
       "      <td>The year's most inventive comedy.</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>David Ansen</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>A winning animated feature that has something ...</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leonard Klady</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Variety</td>\n",
       "      <td>The film sports a provocative and appealing st...</td>\n",
       "      <td>2008-06-09</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jonathan Rosenbaum</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>An entertaining computer-generated, hyperreali...</td>\n",
       "      <td>2008-03-10</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               critic  fresh      imdb     publication  \\\n",
       "0         Derek Adams  fresh  114709.0        Time Out   \n",
       "1     Richard Corliss  fresh  114709.0   TIME Magazine   \n",
       "2         David Ansen  fresh  114709.0        Newsweek   \n",
       "3       Leonard Klady  fresh  114709.0         Variety   \n",
       "4  Jonathan Rosenbaum  fresh  114709.0  Chicago Reader   \n",
       "\n",
       "                                               quote review_date    rtid  \\\n",
       "0  So ingenious in concept, design and execution ...  2009-10-04  9559.0   \n",
       "1                  The year's most inventive comedy.  2008-08-31  9559.0   \n",
       "2  A winning animated feature that has something ...  2008-08-18  9559.0   \n",
       "3  The film sports a provocative and appealing st...  2008-06-09  9559.0   \n",
       "4  An entertaining computer-generated, hyperreali...  2008-03-10  9559.0   \n",
       "\n",
       "       title  \n",
       "0  Toy story  \n",
       "1  Toy story  \n",
       "2  Toy story  \n",
       "3  Toy story  \n",
       "4  Toy story  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Restrict data to reviews with valid ratings and reviews over 10 words long\n",
    "\n",
    "Clean up the reviews, making a column with the case and punctuation removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14072, 8)\n",
      "(14049, 8)\n"
     ]
    }
   ],
   "source": [
    "rt.fresh.unique()\n",
    "print rt.shape \n",
    "rt = rt[rt.fresh.isin(['fresh', 'rotten'])]\n",
    "print rt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt['quote_len'] = rt.quote.map(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "      <th>quote_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12151</th>\n",
       "      <td>Peter Brunette</td>\n",
       "      <td>rotten</td>\n",
       "      <td>160916.0</td>\n",
       "      <td>Film.com</td>\n",
       "      <td>Sloppy.</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>13131.0</td>\n",
       "      <td>The Story of Us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9676</th>\n",
       "      <td>Michael O'Sullivan</td>\n",
       "      <td>rotten</td>\n",
       "      <td>120484.0</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Formulaic!</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>10169.0</td>\n",
       "      <td>The Waterboy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9926</th>\n",
       "      <td>Lisa Schwarzbaum</td>\n",
       "      <td>fresh</td>\n",
       "      <td>120595.0</td>\n",
       "      <td>Entertainment Weekly</td>\n",
       "      <td>Brilliant!</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>9994.0</td>\n",
       "      <td>Babe: Pig in the City</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Rita Kempley</td>\n",
       "      <td>fresh</td>\n",
       "      <td>112346.0</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Frothy.</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>10129.0</td>\n",
       "      <td>The American President</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>fresh</td>\n",
       "      <td>110057.0</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>Unforgettable.</td>\n",
       "      <td>2006-06-24</td>\n",
       "      <td>12741.0</td>\n",
       "      <td>Hoop Dreams</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11429</th>\n",
       "      <td>Todd McCarthy</td>\n",
       "      <td>fresh</td>\n",
       "      <td>167404.0</td>\n",
       "      <td>Variety</td>\n",
       "      <td>Interesting.</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>10054.0</td>\n",
       "      <td>The Sixth Sense</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11788</th>\n",
       "      <td>Robert Horton</td>\n",
       "      <td>rotten</td>\n",
       "      <td>120716.0</td>\n",
       "      <td>Film.com</td>\n",
       "      <td>Sluggish!</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>11619.0</td>\n",
       "      <td>Jakob the Liar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>Geoff Andrew</td>\n",
       "      <td>rotten</td>\n",
       "      <td>110989.0</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>Unendurable.</td>\n",
       "      <td>2006-06-24</td>\n",
       "      <td>10932.0</td>\n",
       "      <td>Ri¢hie Ri¢h</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10104</th>\n",
       "      <td>Lisa Schwarzbaum</td>\n",
       "      <td>fresh</td>\n",
       "      <td>128853.0</td>\n",
       "      <td>Entertainment Weekly</td>\n",
       "      <td>Seductive!</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>10050.0</td>\n",
       "      <td>You've Got Mail</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10710</th>\n",
       "      <td>Lisa Schwarzbaum</td>\n",
       "      <td>fresh</td>\n",
       "      <td>126886.0</td>\n",
       "      <td>Entertainment Weekly</td>\n",
       "      <td>Cool!</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>16574.0</td>\n",
       "      <td>Election</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11508</th>\n",
       "      <td>Susan Wloszczyna</td>\n",
       "      <td>rotten</td>\n",
       "      <td>133046.0</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>Slow-moving.</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>12811.0</td>\n",
       "      <td>Teaching Mrs. Tingle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>Bruce Reid</td>\n",
       "      <td>fresh</td>\n",
       "      <td>113326.0</td>\n",
       "      <td>Film.com</td>\n",
       "      <td>Awe-inspiring.</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>14249.0</td>\n",
       "      <td>Hung fan kui</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10321</th>\n",
       "      <td>Paul Attanasio</td>\n",
       "      <td>rotten</td>\n",
       "      <td>91055.0</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Kitsch.</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>11281.0</td>\n",
       "      <td>Firewalker</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12099</th>\n",
       "      <td>Dave Kehr</td>\n",
       "      <td>rotten</td>\n",
       "      <td>80453.0</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>Witless.</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>14153.0</td>\n",
       "      <td>The Blue Lagoon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8982</th>\n",
       "      <td>Terry Lawson</td>\n",
       "      <td>rotten</td>\n",
       "      <td>119778.0</td>\n",
       "      <td>Detroit Free Press</td>\n",
       "      <td>Predictable!</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>13919.0</td>\n",
       "      <td>Next Stop Wonderland</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13534</th>\n",
       "      <td>Daniel Mangin</td>\n",
       "      <td>rotten</td>\n",
       "      <td>156639.0</td>\n",
       "      <td>Salon.com</td>\n",
       "      <td>Sloppy.</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>14933.0</td>\n",
       "      <td>The Big Tease</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>Wally Hammond</td>\n",
       "      <td>fresh</td>\n",
       "      <td>65421.0</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>Purr-fect.</td>\n",
       "      <td>2006-06-24</td>\n",
       "      <td>9398.0</td>\n",
       "      <td>The AristoCats</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10427</th>\n",
       "      <td>Tom Milne</td>\n",
       "      <td>fresh</td>\n",
       "      <td>84412.0</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>Likeable.</td>\n",
       "      <td>2006-01-26</td>\n",
       "      <td>13736.0</td>\n",
       "      <td>Night Shift</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13530</th>\n",
       "      <td>Stephen Holden</td>\n",
       "      <td>fresh</td>\n",
       "      <td>156639.0</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Charming.</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>14933.0</td>\n",
       "      <td>The Big Tease</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10694</th>\n",
       "      <td>John Anderson</td>\n",
       "      <td>rotten</td>\n",
       "      <td>120797.0</td>\n",
       "      <td>Newsday</td>\n",
       "      <td>Predictable.</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>13817.0</td>\n",
       "      <td>Pushing Tin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   critic   fresh      imdb           publication  \\\n",
       "12151      Peter Brunette  rotten  160916.0              Film.com   \n",
       "9676   Michael O'Sullivan  rotten  120484.0       Washington Post   \n",
       "9926     Lisa Schwarzbaum   fresh  120595.0  Entertainment Weekly   \n",
       "99           Rita Kempley   fresh  112346.0       Washington Post   \n",
       "1314          Derek Adams   fresh  110057.0              Time Out   \n",
       "11429       Todd McCarthy   fresh  167404.0               Variety   \n",
       "11788       Robert Horton  rotten  120716.0              Film.com   \n",
       "1942         Geoff Andrew  rotten  110989.0              Time Out   \n",
       "10104    Lisa Schwarzbaum   fresh  128853.0  Entertainment Weekly   \n",
       "10710    Lisa Schwarzbaum   fresh  126886.0  Entertainment Weekly   \n",
       "11508    Susan Wloszczyna  rotten  133046.0             USA Today   \n",
       "718            Bruce Reid   fresh  113326.0              Film.com   \n",
       "10321      Paul Attanasio  rotten   91055.0       Washington Post   \n",
       "12099           Dave Kehr  rotten   80453.0        Chicago Reader   \n",
       "8982         Terry Lawson  rotten  119778.0    Detroit Free Press   \n",
       "13534       Daniel Mangin  rotten  156639.0             Salon.com   \n",
       "2971        Wally Hammond   fresh   65421.0              Time Out   \n",
       "10427           Tom Milne   fresh   84412.0              Time Out   \n",
       "13530      Stephen Holden   fresh  156639.0        New York Times   \n",
       "10694       John Anderson  rotten  120797.0               Newsday   \n",
       "\n",
       "                quote review_date     rtid                   title  quote_len  \n",
       "12151         Sloppy.  2000-01-01  13131.0         The Story of Us          1  \n",
       "9676       Formulaic!  2000-01-01  10169.0            The Waterboy          1  \n",
       "9926       Brilliant!  2000-01-01   9994.0   Babe: Pig in the City          1  \n",
       "99            Frothy.  2000-01-01  10129.0  The American President          1  \n",
       "1314   Unforgettable.  2006-06-24  12741.0             Hoop Dreams          1  \n",
       "11429    Interesting.  2000-01-01  10054.0         The Sixth Sense          1  \n",
       "11788       Sluggish!  2000-01-01  11619.0          Jakob the Liar          1  \n",
       "1942     Unendurable.  2006-06-24  10932.0             Ri¢hie Ri¢h          1  \n",
       "10104      Seductive!  2000-01-01  10050.0         You've Got Mail          1  \n",
       "10710           Cool!  2000-01-01  16574.0                Election          1  \n",
       "11508    Slow-moving.  2000-01-01  12811.0    Teaching Mrs. Tingle          1  \n",
       "718    Awe-inspiring.  2000-01-01  14249.0            Hung fan kui          1  \n",
       "10321         Kitsch.  2000-01-01  11281.0              Firewalker          1  \n",
       "12099        Witless.  2000-01-01  14153.0         The Blue Lagoon          1  \n",
       "8982     Predictable!  2000-01-01  13919.0    Next Stop Wonderland          1  \n",
       "13534         Sloppy.  2000-01-01  14933.0           The Big Tease          1  \n",
       "2971       Purr-fect.  2006-06-24   9398.0          The AristoCats          1  \n",
       "10427       Likeable.  2006-01-26  13736.0             Night Shift          1  \n",
       "13530       Charming.  2000-01-01  14933.0           The Big Tease          1  \n",
       "10694    Predictable.  2000-01-01  13817.0             Pushing Tin          1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.sort_values('quote_len').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14049, 9)\n",
      "(11215, 9)\n"
     ]
    }
   ],
   "source": [
    "print rt.shape\n",
    "rt = rt[rt.quote_len > 10]\n",
    "print rt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "      <th>quote_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>David Ansen</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>A winning animated feature that has something ...</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leonard Klady</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Variety</td>\n",
       "      <td>The film sports a provocative and appealing st...</td>\n",
       "      <td>2008-06-09</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jonathan Rosenbaum</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>An entertaining computer-generated, hyperreali...</td>\n",
       "      <td>2008-03-10</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Michael Booth</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Denver Post</td>\n",
       "      <td>As Lion King did before it, Toy Story revived ...</td>\n",
       "      <td>2007-05-03</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               critic  fresh      imdb     publication  \\\n",
       "0         Derek Adams  fresh  114709.0        Time Out   \n",
       "2         David Ansen  fresh  114709.0        Newsweek   \n",
       "3       Leonard Klady  fresh  114709.0         Variety   \n",
       "4  Jonathan Rosenbaum  fresh  114709.0  Chicago Reader   \n",
       "5       Michael Booth  fresh  114709.0     Denver Post   \n",
       "\n",
       "                                               quote review_date    rtid  \\\n",
       "0  So ingenious in concept, design and execution ...  2009-10-04  9559.0   \n",
       "2  A winning animated feature that has something ...  2008-08-18  9559.0   \n",
       "3  The film sports a provocative and appealing st...  2008-06-09  9559.0   \n",
       "4  An entertaining computer-generated, hyperreali...  2008-03-10  9559.0   \n",
       "5  As Lion King did before it, Toy Story revived ...  2007-05-03  9559.0   \n",
       "\n",
       "       title  quote_len  \n",
       "0  Toy story         24  \n",
       "2  Toy story         13  \n",
       "3  Toy story         17  \n",
       "4  Toy story         14  \n",
       "5  Toy story         40  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep = string.ascii_lowercase + ' -'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>quote_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>so ingenious in concept design and execution t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A winning animated feature that has something ...</td>\n",
       "      <td>a winning animated feature that has something ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The film sports a provocative and appealing st...</td>\n",
       "      <td>the film sports a provocative and appealing st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An entertaining computer-generated, hyperreali...</td>\n",
       "      <td>an entertaining computer-generated hyperrealis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>As Lion King did before it, Toy Story revived ...</td>\n",
       "      <td>as lion king did before it toy story revived t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The film will probably be more fully appreciat...</td>\n",
       "      <td>the film will probably be more fully appreciat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Children will enjoy a new take on the irresist...</td>\n",
       "      <td>children will enjoy a new take on the irresist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Although its computer-generated imagery is imp...</td>\n",
       "      <td>although its computer-generated imagery is imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How perfect that two of the most popular funny...</td>\n",
       "      <td>how perfect that two of the most popular funny...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Disney's witty, wondrously imaginative, all-co...</td>\n",
       "      <td>disneys witty wondrously imaginative all-compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The script, by Lasseter, Pete Docter, Andrew S...</td>\n",
       "      <td>the script by lasseter pete docter andrew stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>It's a nice change of pace to see the studio d...</td>\n",
       "      <td>its a nice change of pace to see the studio dr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                quote  \\\n",
       "0   So ingenious in concept, design and execution ...   \n",
       "2   A winning animated feature that has something ...   \n",
       "3   The film sports a provocative and appealing st...   \n",
       "4   An entertaining computer-generated, hyperreali...   \n",
       "5   As Lion King did before it, Toy Story revived ...   \n",
       "6   The film will probably be more fully appreciat...   \n",
       "7   Children will enjoy a new take on the irresist...   \n",
       "8   Although its computer-generated imagery is imp...   \n",
       "9   How perfect that two of the most popular funny...   \n",
       "11  Disney's witty, wondrously imaginative, all-co...   \n",
       "13  The script, by Lasseter, Pete Docter, Andrew S...   \n",
       "16  It's a nice change of pace to see the studio d...   \n",
       "\n",
       "                                         quote_parsed  \n",
       "0   so ingenious in concept design and execution t...  \n",
       "2   a winning animated feature that has something ...  \n",
       "3   the film sports a provocative and appealing st...  \n",
       "4   an entertaining computer-generated hyperrealis...  \n",
       "5   as lion king did before it toy story revived t...  \n",
       "6   the film will probably be more fully appreciat...  \n",
       "7   children will enjoy a new take on the irresist...  \n",
       "8   although its computer-generated imagery is imp...  \n",
       "9   how perfect that two of the most popular funny...  \n",
       "11  disneys witty wondrously imaginative all-compu...  \n",
       "13  the script by lasseter pete docter andrew stan...  \n",
       "16  its a nice change of pace to see the studio dr...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt['quote_parsed'] = rt.quote.map(lambda x: ''.join([ch for ch in x.lower() if ch in keep]))\n",
    "rt[['quote', 'quote_parsed']].head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Write a function to assign positive rating, negative, and objective based on words in review\n",
    "\n",
    "We'll use the dictionary we constructed above (without the part of speech tags). \n",
    "\n",
    "Objectivity is calculated: \n",
    "\n",
    "    1. - (positive_score + negative_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scorer(pq):\n",
    "    words = pq.split()\n",
    "    pos, neg, obj = [],[],[]\n",
    "    for word in words:\n",
    "        try:\n",
    "            pos.append(agg_dict[word]['pos_score'])\n",
    "            neg.append(agg_dict[word]['neg_score'])\n",
    "            obj_rating = 1. - (pos[-1] + neg[-1])\n",
    "            obj.append(obj_rating)\n",
    "        except:\n",
    "            pos.append(0.)\n",
    "            neg.append(0.)\n",
    "            obj.append(1.)\n",
    "    return pos, neg, obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agg_scores = map(scorer, rt.quote_parsed.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'capraesque has inevitably been affixed to the american president but that phrase with its implications of facile hokum doesnt do the film justice'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt['quote_parsed'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.025000000000000001,\n",
       "  0.0625,\n",
       "  0.017857142857150003,\n",
       "  0.0,\n",
       "  0.16369047619050001,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.017857142857099998,\n",
       "  0.0,\n",
       "  0.25,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1875,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.70833333333299997,\n",
       "  0.035714285714300006,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.046180555555550007,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.041666666666666664,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.017857142857099998,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.017857142857099998,\n",
       "  0.15625,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.20833333333299997,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.25,\n",
       "  0.0029761904761900003,\n",
       "  0.0,\n",
       "  0.050595238095200001,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.035714285714300006,\n",
       "  0.0,\n",
       "  0.5,\n",
       "  0.25,\n",
       "  0.0,\n",
       "  0.28125,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.017857142857150003,\n",
       "  0.0,\n",
       "  0.03125,\n",
       "  0.0625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.098958333333325002,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.19444444444433331,\n",
       "  0.875,\n",
       "  0.0,\n",
       "  0.035714285714300006,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0625,\n",
       "  0.0,\n",
       "  0.035714285714300006,\n",
       "  0.010416666666649999,\n",
       "  0.0],\n",
       " [1.0,\n",
       "  1.0,\n",
       "  0.79166666666700003,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.97499999999999998,\n",
       "  0.6875,\n",
       "  0.97916666666665997,\n",
       "  1.0,\n",
       "  0.78571428571430002,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.94642857142860004,\n",
       "  1.0,\n",
       "  0.25,\n",
       "  0.75,\n",
       "  1.0,\n",
       "  0.53125,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.29166666666700003,\n",
       "  0.94642857142854997,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.85486111111112506,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.76388888888900008,\n",
       "  0.125,\n",
       "  1.0,\n",
       "  0.94642857142860004,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9375,\n",
       "  1.0,\n",
       "  0.94642857142860004,\n",
       "  0.83333333333335002,\n",
       "  1.0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_scores[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Calculate the sum and average ratings for positive, negative, and objective for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt['pos_avg'] = [np.mean(x[0]) for x in agg_scores]\n",
    "rt['neg_avg'] = [np.mean(x[1]) for x in agg_scores]\n",
    "rt['obj_avg'] = [np.mean(x[2]) for x in agg_scores]\n",
    "\n",
    "rt['pos_sum'] = [np.sum(x[0]) for x in agg_scores]\n",
    "rt['neg_sum'] = [np.sum(x[1]) for x in agg_scores]\n",
    "rt['obj_sum'] = [np.sum(x[2]) for x in agg_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appropriately operatic chens visually spectacular epic is sumptuous in every respect intelligent enthralling rhapsodic\n",
      "-------------------------\n",
      "\n",
      "hilarious sexy clever playful and as initially teasing as it is ultimately satisfying\n",
      "-------------------------\n",
      "\n",
      "a very good film with some dazzling moments and one truly outstanding performance\n",
      "-------------------------\n",
      "\n",
      "remains a beautiful deftly directed and superbly acted version of a witty and poignant drama\n",
      "-------------------------\n",
      "\n",
      "all the excellent creative components do not add up to a whole\n",
      "-------------------------\n",
      "\n",
      "from russia with love is a preposterous skillful slab of hardhitting sexy hokum\n",
      "-------------------------\n",
      "\n",
      "the karate kid exhibits warmth and friendly predictable humor its greatest assets\n",
      "-------------------------\n",
      "\n",
      "sophisticated well not really but fast smart shrewdly directed and capably performed\n",
      "-------------------------\n",
      "\n",
      "part homage part spoof the deft balancing act is a clever engaging adaption\n",
      "-------------------------\n",
      "\n",
      "an ingenious script excellent special effects and photography and superior acting make it an endearing winner\n",
      "-------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rt.sort_values('pos_avg', ascending=False, inplace=True)\n",
    "for i in range(10):\n",
    "    print rt.quote_parsed.values[i]\n",
    "    print '-------------------------\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluate predictive ability using the sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = rt[['pos_avg', 'neg_avg', 'obj_avg', 'quote_len']]\n",
    "y = rt.fresh.map(lambda x: 1 if x == 'fresh' else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61506910387873381"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.61853832  0.62566845  0.63458111  0.63101604  0.63368984  0.63368984\n",
      "  0.63101604  0.65120428  0.60714286  0.51875   ]\n",
      "0.618529678253\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "Xn = StandardScaler().fit_transform(X)\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(), Xn, y, cv=10)\n",
    "print scores\n",
    "print np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Import spacy\n",
    "\n",
    "The spacy package is the current gold standard for parsing text. We are going to use it to find the part of speech tags for the review words. \n",
    "\n",
    "Once we have parsed the tags with spacey, we can assign sentiment scores at a more granular level, using the correct part of speech version of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "en_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Parse the quotes using spacey's multithreaded parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Create columns for part of speech proportions\n",
    "\n",
    "For each of the part of speech tags, create a column in the dataset that records the proportion of words in the quote that have that part of speech tag. We can try using these as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluate a model with the new part of speech predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Print out the most likely fresh and most likely rotten reviews\n",
    "\n",
    "Using the predicted probabilities from our model, we can see which reviews are most likely to be fresh or rotten. We can easily validate that our model is doing something that makes sense by looking at these (one of the benefits of doing NLP work!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Assign sentiment scores using the correct part of speech tag\n",
    "\n",
    "We need to write another function that will take into account the part of speech tags using the parsed quotes we created earlier and the original sentiment data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluate the new predictors with different models.\n",
    "\n",
    "Does regularization help? Decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
