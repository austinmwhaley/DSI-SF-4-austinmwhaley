{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "import multiprocessing\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "\n",
    "msg_success = \"Program Execution Complete\"\n",
    "msg_error = \"Error When Executing Program\"\n",
    "textfile = '/Users/austinwhaley/github_repos/DSI-SF-4-austinmwhaley/other_datasets/nn_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### EMPTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d_from_263_Grand_Central</th>\n",
       "      <th>d_from_263_Times_Square</th>\n",
       "      <th>d_from_263_Broadway</th>\n",
       "      <th>d_from_263_Citi_Field</th>\n",
       "      <th>d_from_263_MetLife_Stadiumn</th>\n",
       "      <th>d_from_263_Barclays_Center</th>\n",
       "      <th>d_from_263_Yankees_Stadium</th>\n",
       "      <th>d_from_263_MSG</th>\n",
       "      <th>d_from_262_Grand_Central</th>\n",
       "      <th>d_from_262_Times_Square</th>\n",
       "      <th>...</th>\n",
       "      <th>263</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>Dew PointF</th>\n",
       "      <th>Gust SpeedMPH</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Sea Level PressureIn</th>\n",
       "      <th>TemperatureF</th>\n",
       "      <th>VisibilityMPH</th>\n",
       "      <th>Wind SpeedMPH</th>\n",
       "      <th>WindDirDegrees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>10.1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>30.19</td>\n",
       "      <td>19.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 2380 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   d_from_263_Grand_Central  d_from_263_Times_Square  d_from_263_Broadway  \\\n",
       "0                       4.3                      4.5                  3.2   \n",
       "\n",
       "   d_from_263_Citi_Field  d_from_263_MetLife_Stadiumn  \\\n",
       "0                   10.1                         17.0   \n",
       "\n",
       "   d_from_263_Barclays_Center  d_from_263_Yankees_Stadium  d_from_263_MSG  \\\n",
       "0                        11.0                         6.0             5.5   \n",
       "\n",
       "   d_from_262_Grand_Central  d_from_262_Times_Square       ...         263  \\\n",
       "0                       2.3                      2.8       ...        59.0   \n",
       "\n",
       "   DAY_OF_WEEK  Dew PointF  Gust SpeedMPH  Humidity  Sea Level PressureIn  \\\n",
       "0       Sunday         6.1            0.0      55.0                 30.19   \n",
       "\n",
       "   TemperatureF  VisibilityMPH  Wind SpeedMPH  WindDirDegrees  \n",
       "0          19.9           10.0            0.0               0  \n",
       "\n",
       "[1 rows x 2380 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber = pd.read_csv('/Users/austinwhaley/github_repos/DSI-SF-4-austinmwhaley/other_datasets/uber_data5.csv').drop(['Unnamed: 0', 'locationID'], 1)\n",
    "uber.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Monday = pd.get_dummies(uber[['DAY_OF_WEEK']])['DAY_OF_WEEK_Monday']\n",
    "Tuesday = pd.get_dummies(uber[['DAY_OF_WEEK']])['DAY_OF_WEEK_Tuesday']\n",
    "Wednesday = pd.get_dummies(uber[['DAY_OF_WEEK']])['DAY_OF_WEEK_Wednesday']\n",
    "Thursday = pd.get_dummies(uber[['DAY_OF_WEEK']])['DAY_OF_WEEK_Thursday']\n",
    "Friday = pd.get_dummies(uber[['DAY_OF_WEEK']])['DAY_OF_WEEK_Friday']\n",
    "Saturday = pd.get_dummies(uber[['DAY_OF_WEEK']])['DAY_OF_WEEK_Saturday']\n",
    "Sunday = pd.get_dummies(uber[['DAY_OF_WEEK']])['DAY_OF_WEEK_Sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "counter = 2371\n",
    "for i in [Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday]:\n",
    "    counter += 1\n",
    "    uber.insert(counter, str(i.name), i)\n",
    "print 'DONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i, j in enumerate(uber.columns):\n",
    "#     print i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = []\n",
    "columns.extend(uber.iloc[:, 2104:2108].columns) #Time\n",
    "columns.extend(uber.iloc[:,2379:].columns)      #Weather\n",
    "#columns.extend(uber.iloc[:,:2104].columns)      #d_from\n",
    "columns.extend(uber.iloc[:,2371:2379].columns)  #DAY_OF_WEEK\n",
    "columns.extend(uber.iloc[:,2108:2371].columns)  #labels\n",
    "uber = uber.reindex(columns=columns).drop('DAY_OF_WEEK', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Dew PointF</th>\n",
       "      <th>Gust SpeedMPH</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Sea Level PressureIn</th>\n",
       "      <th>TemperatureF</th>\n",
       "      <th>VisibilityMPH</th>\n",
       "      <th>...</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>30.19</td>\n",
       "      <td>19.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 282 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  Day  Hour  Dew PointF  Gust SpeedMPH  Humidity  \\\n",
       "0  2015      2    1     0         6.1            0.0      55.0   \n",
       "\n",
       "   Sea Level PressureIn  TemperatureF  VisibilityMPH  ...   254    255   256  \\\n",
       "0                 30.19          19.9           10.0  ...   2.0  158.0  82.0   \n",
       "\n",
       "    257  258  259  260   261   262   263  \n",
       "0  10.0  1.0  2.0  8.0  18.0  20.0  59.0  \n",
       "\n",
       "[1 rows x 282 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i, j in enumerate(uber.columns):\n",
    "#       print i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3599, 282)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#uber.to_csv('/Users/austinwhaley/github_repos/DSI-SF-4-austinmwhaley/uber-pickups-in-new-york-city/uber_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3599 entries, 0 to 3598\n",
      "Columns: 282 entries, Year to 263\n",
      "dtypes: float64(277), int64(5)\n",
      "memory usage: 7.7 MB\n"
     ]
    }
   ],
   "source": [
    "uber.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3598 3598\n"
     ]
    }
   ],
   "source": [
    "#Scale data between 0,1 for LSTM/GRU\n",
    "MMS = MinMaxScaler()\n",
    "uber = pd.DataFrame(MMS.fit_transform(uber), columns=uber.columns)\n",
    "\n",
    "#  Remember to shift the data so we are predicting one time step future\n",
    "X = uber.iloc[:-1,:]\n",
    "y = uber.iloc[1:,-263:] #263 locationIDs\n",
    "\n",
    "# Train-Test-Split-90/10\n",
    "X_train, X_test = X[:int(round(len(X)*0.8, 0))], X[int(round(len(X)*0.8, 0)):]\n",
    "y_train, y_test = y[:int(round(len(y)*0.8, 0))], y[int(round(len(y)*0.8, 0)):]\n",
    "\n",
    "#Reshape for LSTM/GRU layers\n",
    "Xtr = X_train.values.reshape((1, X_train.shape[0], X_train.shape[1]))\n",
    "ytr = y_train.values.reshape((1, y_train.shape[0], y_train.shape[1]))\n",
    "Xte = X_test.values.reshape((1, X_test.shape[0], X_test.shape[1]))\n",
    "yte = y_test.values.reshape((1, y_test.shape[0], y_test.shape[1]))\n",
    "\n",
    "#Check data validity\n",
    "print len(X_train) + len(X_test), len(y_train) + len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "#K._BACKEND == 'tensorflow'\n",
    "\n",
    "def custom_r2(y_true, y_pred):\n",
    "    baseline = K.sum((y_true - K.mean(y_true))**2)\n",
    "    model_fit = K.sum((y_true - y_pred)**2)\n",
    "    return 1. - model_fit/baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, GRU, TimeDistributed, Input, merge, Activation\n",
    "from keras.layers import Reshape\n",
    "from keras.optimizers import rmsprop, Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(n_features, dropout, n_outputs, compile_model=1, load_weights=0, loss='mse', lr=0.01):\n",
    "    '''\n",
    "    Builds a n-Layer neural_network \n",
    "    \n",
    "    Parmaters\n",
    "    ---------\n",
    "    \n",
    "    '''\n",
    "    model = Sequential()\n",
    "\n",
    "    #DENSE - INPUT\n",
    "    model.add(TimeDistributed(Dense(256, activation='relu'), input_shape=(None, n_features)))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    #DENSE (2 Layers)\n",
    "    model.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    #LSTM (1 Layers)\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "    #DENSE (2 Layers)\n",
    "    model.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # LSTM (1 Layer)\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # DENSE (2 Layers)\n",
    "    model.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    #OUTPUT\n",
    "    model.add(TimeDistributed(Dense(n_outputs, activation='relu')))\n",
    "\n",
    "    if compile_model == 1:\n",
    "        model.compile(loss=loss, optimizer=Adamax(lr=lr), metrics=[custom_r2])\n",
    "\n",
    "    if load_weights == 1:\n",
    "        model.load_weights('/Users/austinwhaley/github_repos/DSI-SF-4-austinmwhaley/other_datasets/weights_'+file_name+'.h5')\n",
    "        \n",
    "    return model\n",
    "    \n",
    "    \n",
    "def run_neural_network(model, Xtr, ytr, Xte, yte, n_epochs, batch_size=1, verbose=0, save_weights=0):\n",
    "    '''\n",
    "    Runs neural_network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    '''\n",
    "    try: \n",
    "        start_time = time.time()\n",
    "        print 'Executing ||', str(datetime.now()) \n",
    "\n",
    "        history = model.fit(Xtr, ytr, validation_data=(Xte, yte),\n",
    "                  batch_size=1, verbose=verbose, nb_epoch=n_epochs, shuffle=False)\n",
    "\n",
    "        print 'Execution_Complete || Runtime w/', n_epochs, 'epochs =', round((time.time() - start_time)/60., 2), 'minutes'\n",
    "\n",
    "        if save_weights == 1:\n",
    "            save_unit_weights()\n",
    "\n",
    "        return history\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    except:\n",
    "        print '--Error in running model---'\n",
    "    \n",
    "    \n",
    "def save_unit_weights():\n",
    "    '''\n",
    "    Saves weights of previously run neural_network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    '''\n",
    "    file_name = str(datetime.now()).replace(':', '_').replace(' ','_')[:19]\n",
    "    model.save_weights('/Users/austinwhaley/github_repos/DSI-SF-4-austinmwhaley/other_datasets/weights_'+file_name+'.h5', overwrite=True)\n",
    "    print 'Saved ||', str(datetime.now())\n",
    "    \n",
    "    \n",
    "def plot_neural_net_results(history, limit_r2=0, save_results=0):\n",
    "    '''\n",
    "    Plots results of neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    '''\n",
    "    results = pd.DataFrame(history.history)\n",
    "    #results = pd.read_csv('/Users/austinwhaley/Desktop/DSI-SF-4-austinmwhaley/other_datasets/uber_results_mse_attn_1000').drop('Unnamed: 0', 1)\n",
    "\n",
    "    # SUMMARY RESULTS\n",
    "    print '---MAX RESULTS--- \\n', results[['custom_r2','val_custom_r2']].iloc[:].max(), '\\n'\n",
    "    print '---TAIL RESULTS--- \\n', results[['custom_r2','val_custom_r2']].iloc[-1:], '\\n'\n",
    "\n",
    "    # Build figure\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12,4.5))\n",
    "\n",
    "    # Assign Axis Plots\n",
    "    ax0.plot(results[['custom_r2', 'val_custom_r2']].iloc[:])\n",
    "    ax1.plot(results[['loss', 'val_loss']].iloc[:])\n",
    "\n",
    "    # Add horizontal and vertical line for max\n",
    "    ax0.axhline(results['val_custom_r2'].max(), color='red', lw=0.5)\n",
    "    ax0.axvline(results['val_custom_r2'].idxmax(), color='red', lw=0.5)\n",
    "\n",
    "    #Set axis limits\n",
    "    if limit_r2 == 1:\n",
    "        ax0.set_ylim(bottom=0.5, top=0.6)\n",
    "\n",
    "    # Rename axis titles\n",
    "    ax0.set_title('r2', fontsize=15)\n",
    "    ax1.set_title('loss', fontsize=15)\n",
    "\n",
    "    #ax0.legend(loc='lower right')\n",
    "\n",
    "    # Show plots\n",
    "    plt.show()\n",
    "\n",
    "    if save_results == 1:\n",
    "        file_name = str(datetime.now()).replace(':', '_').replace(' ','_')[:19]\n",
    "        results.to_csv('/Users/austinwhaley/github_repos/DSI-SF-4-austinmwhaley/other_datasets/results_'+ file_name + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Run a 3000 epoch dense only w/ venue data - DONE\n",
    "# Run a 3000 epoch dense only w/o venue data - DONE\n",
    "# Compare results\n",
    "\n",
    "# Run a single layer LSTM\n",
    "# Run a duel layer LSTM\n",
    "\n",
    "# Finalize model with winner of this A|B testing\n",
    "# Clean other_datasets\n",
    "\n",
    "# Finalize predictions (round to int)\n",
    "# Get started with the visualization (test)\n",
    "\n",
    "# Given a start loc (lon, lat) & taking into account drive time, what is the best place to drive too (real time aspect)\n",
    "# '''\n",
    "# 1 - 7L = In(n_features)/D(512)/D(512)/D(512)/D(512)/D(512)/Out(n_outputs):\n",
    "# w/ Venue Data (3599, 2386)\n",
    "# 5000 epochs MAX=0.5843. RUNTIME=144.72min\n",
    "# NOTES: This one seemed pretty much good to go when it came to training results\n",
    "\n",
    "\n",
    "# 2 - 7L = In(n_features)/D(512)/D(512)/D(512)/D(512)/D(512)/Out(n_outputs):\n",
    "# w/o Venue Data (3599, 282)\n",
    "# 5000 epochs MAX=0.5790. RUNTIME=102.36min\n",
    "# NOTES: I think this one could have gotten slightly better given additional time\n",
    "\n",
    "\n",
    "# 3 - 10L = In(n_features)/D(512)/D(512)/LSTM(512)/D(512)/D(512)/LSTM(512)/D(512)/D(512)/Out(n_outputs)\n",
    "# w/ Venue Data (3599, 2386)\n",
    "# 2000 epochs MAX=0.0000. RUNTIME=000.00min\n",
    "# NOTES: \n",
    "\n",
    "\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#READY?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing || 2017-02-06 11:50:59.702953\n"
     ]
    }
   ],
   "source": [
    "#1 - Build Model\n",
    "model = build_model(n_features=Xtr.shape[2], dropout=0.5, n_outputs=263, compile_model=1, load_weights=0, lr=0.005)\n",
    "\n",
    "#2 - Run Model\n",
    "history = run_neural_network(model=model, Xtr=Xtr, ytr=ytr, Xte=Xte, yte=yte, save_weights=1, verbose=0, n_epochs=4000)\n",
    "\n",
    "#3 - Plot Model\n",
    "plot_neural_net_results(history, save_results=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scores = r2_score(y_test.values, predictions.values, multioutput='raw_values')\n",
    "# [round(x*1000)/1000. for x in scores]\n",
    "\n",
    "# for i, x in enumerate(scores):\n",
    "#     print i, round(x*1000)/1000.\n",
    "#     counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2878 predictions (0.55) seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "predictions = model.predict(Xtr)\n",
    "predictions = pd.DataFrame(predictions.reshape(predictions.shape[1], predictions.shape[2]))\n",
    "print predictions.shape[0], 'predictions', '('+ str(round((time.time() - start_time), 2)) +') seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.725290e-09</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.250169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.275946</td>\n",
       "      <td>0.017392</td>\n",
       "      <td>0.111403</td>\n",
       "      <td>0.100017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13809</td>\n",
       "      <td>0.223976</td>\n",
       "      <td>0.268088</td>\n",
       "      <td>0.147001</td>\n",
       "      <td>0.162886</td>\n",
       "      <td>0.113185</td>\n",
       "      <td>0.205238</td>\n",
       "      <td>0.207096</td>\n",
       "      <td>0.124798</td>\n",
       "      <td>0.220128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 263 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0             1         2         3    4    5         6         7    \\\n",
       "0  0.0  3.725290e-09  0.103437  0.250169  0.0  0.0  0.275946  0.017392   \n",
       "\n",
       "        8         9      ...         253       254       255       256  \\\n",
       "0  0.111403  0.100017    ...     0.13809  0.223976  0.268088  0.147001   \n",
       "\n",
       "        257       258       259       260       261       262  \n",
       "0  0.162886  0.113185  0.205238  0.207096  0.124798  0.220128  \n",
       "\n",
       "[1 rows x 263 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inverse_transform(x):\n",
    "    lst = []\n",
    "    counter = 0\n",
    "    for i in x:\n",
    "        i -= MMS.data_min_[19 + counter]\n",
    "        i /= MMS.scale_[19 + counter]\n",
    "        lst.append(int(i))\n",
    "        #print counter\n",
    "        if counter != 262:\n",
    "            counter += 1\n",
    "        else:\n",
    "            counter = 0\n",
    "            \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = predictions.apply(inverse_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 263 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9   ...   253  254  255  256  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "2    0    0    0    1    0    0    1    0    0    0 ...     0    1    1    0   \n",
       "3    0    0    6   11    0    0   14    0    5    6 ...     8   10   12    5   \n",
       "4    0    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "\n",
       "   257  258  259  260  261  262  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    1    0    1    0    0    0  \n",
       "3    9    6   12    7    3    7  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 263 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualization\n",
    "# Step 1: Get locations for 265 location_ids DONE\n",
    "# Step 2: Get predictions for 265 location_ids for given time DONE\n",
    "# Step 3: Plot predictions on map\n",
    "# Step 4: Rule the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zones = pd.read_csv('/Users/austinwhaley/github_repos/DSI-SF-4-austinmwhaley/other_datasets/taxi-zone-pickup3.csv').drop('Unnamed: 0', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latitude, longitude = [], []\n",
    "hours_away = 3\n",
    "for x, y in zip(predictions.ix[hours_away,:], zip(zones['Latitude'], zones['Longitude'])):\n",
    "    for i in range(0, x):\n",
    "        latitude.append(y[0])\n",
    "        longitude.append(y[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gmplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_lat, start_lng, zoom = 40.7128, -74.0059, 10\n",
    "gmap = gmplot.GoogleMapPlotter(start_lat, start_lng, zoom)\n",
    "gmap.heatmap(latitude, longitude, threshold=1, opacity=3.0, radius=25)\n",
    "gmap.draw('/Users/austinwhaley/Desktop/uber_heatmap.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### EMPTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  ARCHIVE: Used to predict r2 on STATEFUL model\n",
    "\n",
    "# #Go row by row (batch) and make a prediction\n",
    "# def r2(model):\n",
    "#     predictions = []\n",
    "\n",
    "#     for i in range(len(X_test)):\n",
    "#         #prediction = model.predict_on_batch(X_test.values[i:i+1,:])\n",
    "#         prediction = model.predict_on_batch(np.reshape(X_test.values[i:i+1,:], (1, 1, X_train.shape[1])))\n",
    "#         predictions.append(np.reshape(prediction, (265,)))\n",
    "\n",
    "#     predictions = pd.DataFrame(predictions)\n",
    "#     predictions.columns = range(1,266)\n",
    "#     #predictions\n",
    "\n",
    "#     score = r2_score(y_test.values, predictions.values, multioutput='uniform_average')\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  ARCHIVE: STATEFUL model\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# #TimeDistributedDense (2 Layers)\n",
    "# model.add(TimeDistributedDense(256, activation='relu', batch_input_shape=(1, 1, 277)))\n",
    "# model.add(Dropout(0.0)) #Dropout value should be 0.5 on production run\n",
    "# model.add(TimeDistributedDense(256, activation='relu'))\n",
    "# model.add(Dropout(0.0))\n",
    "# #model.add(Reshape(target_shape=(1, 32)))\n",
    "\n",
    "# #LSTM (2 Layers)\n",
    "# model.add(LSTM(256, return_sequences=True, stateful=True, consume_less='cpu')) #batch_input_shape=(1,1,277)\n",
    "# model.add(Dropout(0.0))\n",
    "# model.add(LSTM(256, return_sequences=False, stateful=True, consume_less='cpu'))\n",
    "# model.add(Dropout(0.0))\n",
    "\n",
    "# #Dense (1 Layer)\n",
    "# #model.add(TimeDistributedDense(64, activation='linear'))\n",
    "# #model.add(Dropout(0.00))\n",
    "# model.add(Dense(265, activation='relu'))\n",
    "# model.compile(loss='mse', optimizer='adam', metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ARCHIVE: This code was used to train the STATEFUL v. of the model\n",
    "\n",
    "# model.reset_states()\n",
    "# try:    \n",
    "#     epochs = 20\n",
    "#     r2s = []\n",
    "\n",
    "#     print('Training...')\n",
    "#     for epoch in range(epochs):\n",
    "#         start_time = time.time()\n",
    "#         mean_tr_acc = []\n",
    "#         mean_tr_loss = []\n",
    "#         for i in range(len(X_train)): #Iterate through each row\n",
    "#             #tr_loss, tr_acc = model.train_on_batch(X_train.values[i:i+1,:], y_train.values[i:i+1,:])\n",
    "\n",
    "#             tr_loss, tr_acc = model.train_on_batch(np.reshape(X_train.values[i:i+1,:], (1, 1, X_train.shape[1])),\n",
    "#                                                     np.reshape(y_train.values[i:i+1,:], (1, y_train.shape[1])))\n",
    "\n",
    "#             mean_tr_acc.append(tr_acc)\n",
    "#             mean_tr_loss.append(tr_loss)\n",
    "\n",
    "\n",
    "#         model.reset_states() #Why do we reset the model twice here?\n",
    "#         rsquared = r2(model)\n",
    "#         r2s.append(rsquared)\n",
    "#         model.reset_states()\n",
    "#         print('epoch = {} || r2 = {} || loss = {} || mse = {} || exe_time = {}').format(epoch, \n",
    "#                                                                                         rsquared,\n",
    "#                                                                                         np.mean(mean_tr_acc),\n",
    "#                                                                                         np.mean(mean_tr_loss),\n",
    "#                                                                                         (time.time() - start_time))\n",
    "    \n",
    "#     print 'Training_Complete...' \n",
    "#     pd.DataFrame(r2s).to_csv('/Users/austinwhaley/Desktop/DSI-SF-4-austinmwhaley/other_datasets/r2s_nn.csv')\n",
    "#     send_email(email_address, password, msg_success)\n",
    "# except KeyboardInterrupt:\n",
    "#     print '--KeyboardInterrupt--'\n",
    "# except:\n",
    "#     print '--An Error Has Occured!---'\n",
    "#     model.reset_states()\n",
    "#     pd.DataFrame(r2s).to_csv('/Users/austinwhaley/Desktop/DSI-SF-4-austinmwhaley/other_datasets/r2s_nn.csv')\n",
    "#     send_email(email_address, password, msg_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  ARCHIVE: This code was used to test the STATEFUL v. of the model\n",
    "\n",
    "# epochs = 1\n",
    "\n",
    "# print('Testing...')\n",
    "# for epoch in range(epochs):\n",
    "#     start_time = time.time()\n",
    "#     mean_tr_acc = []\n",
    "#     mean_tr_loss = []\n",
    "#     for i in range(len(X_test)): #Iterate through each row\n",
    "#         tr_loss, tr_acc = model.test_on_batch(np.reshape(np.array(X_test)[i], (1, 1, X_test.shape[1])), \n",
    "#                                                np.reshape(np.array(y_test)[i], (1, y_test.shape[1])))\n",
    "#         mean_tr_acc.append(tr_acc)\n",
    "#         mean_tr_loss.append(tr_loss)\n",
    "\n",
    "#     model.reset_states()\n",
    "#     print('epoch = {} || loss = {} || mse = {} || exe_time = {}').format(epoch,\n",
    "#                                                                          np.mean(mean_tr_acc),\n",
    "#                                                                          np.mean(mean_tr_loss),\n",
    "#                                                                          (time.time() - start_time))\n",
    "#     #print ('___________________________________')\n",
    "# print 'Testing_Complete...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ARCHIVE: This model is the sequential model that was first build, succeeded by the functional model\n",
    "\n",
    "#This is the second model (stateless) that Keifer and I built\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(TimeDistributed(Dense(512, activation='relu'), input_shape=(None, 277)))#X_train.shape[0]/2, 277)))\n",
    "# model.add(Dropout(0.5)) #Dropout value should be 0.5 on production run\n",
    "# model.add(TimeDistributed(Dense(512, activation='relu')))\n",
    "# model.add(Dropout(0.5))\n",
    "# #model.add(Reshape(target_shape=(1, 32)))\n",
    "\n",
    "# #LSTM (2 Layers)\n",
    "# model.add(LSTM(1024, return_sequences=True, stateful=False, consume_less='cpu')) #batch_input_shape=(1,1,277)\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(LSTM(1024, return_sequences=True, stateful=False, consume_less='cpu'))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# #Dense (1 Layer)\n",
    "# #model.add(TimeDistributedDense(64, activation='linear'))\n",
    "# #model.add(Dropout(0.00))\n",
    "# model.add(TimeDistributed(Dense(265, activation='relu')))\n",
    "# model.compile(loss='mse', optimizer='rmsprop', metrics=[custom_r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ARCHIVE: First Attention model\n",
    "\n",
    "# inputs = Input((None, X_train.shape[1]))\n",
    "\n",
    "# m = TimeDistributed(Dense(512, activation='relu'))(inputs)\n",
    "# m = Dropout(0.5)(m)\n",
    "# m = TimeDistributed(Dense(512, activation='relu'))(m)\n",
    "# m = Dropout(0.5)(m)\n",
    "# m = TimeDistributed(Dense(512, activation='relu'))(m)\n",
    "# m = Dropout(0.5)(m)\n",
    "\n",
    "# gru = GRU(512, return_sequences=True, stateful=False, consume_less='cpu')(m)\n",
    "# gru = Dropout(0.5)(gru)\n",
    "\n",
    "# attn = GRU(512, return_sequences=True, stateful=False, consume_less='cpu')(m)\n",
    "# attn = Activation('softmax')(attn)\n",
    "\n",
    "# gru_attn = merge([gru, attn], mode='mul')\n",
    "\n",
    "# mix = TimeDistributed(Dense(512, activation='relu'))(gru_attn)\n",
    "# mix = Dropout(0.5)(mix)\n",
    "\n",
    "# out = TimeDistributed(Dense(265, activation='relu'))(mix)\n",
    "# model = Model(input=inputs, output=out)\n",
    "# model.compile(loss='mse', optimizer=Adamax(lr=0.001), metrics=[custom_r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How to import from file not in directory\n",
    "# import imp\n",
    "# split_test = imp.load_source('split_test', '/Users/austinwhaley/github_repos/DSI-SF-4-austinmwhaley/utils/split_test/split_test_generator.py')\n",
    "# from split_test import SplitTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ARCHIVE: NON-CLASS EXECUTION\n",
    "# try:\n",
    "#     n_epochs = 10\n",
    "#     start_time = time.time()\n",
    "#     print 'Executing ||', str(datetime.now()) \n",
    "\n",
    "#     history = model.fit(Xtr, ytr, validation_data=(Xte, yte),\n",
    "#               batch_size=1, verbose=0, nb_epoch=n_epochs, shuffle=False)\n",
    "\n",
    "#     print 'Execution_Complete...' \n",
    "#     #pd.DataFrame(history.history)['val_custom_r2'].to_csv('/Users/austinwhaley/Desktop/DSI-SF-4-austinmwhaley/other_datasets/nn_results.csv')\n",
    "#     #send_email(email_address, password, msg_success)\n",
    "#     print 'Runtime w/', n_epochs, 'epochs =', round((time.time() - start_time)/60., 2), 'minutes'   \n",
    "# except KeyboardInterrupt:\n",
    "#     pass\n",
    "# except:\n",
    "#     print '--An Error Has Occured!---'\n",
    "#     #pd.DataFrame(history.history)['val_custom_r2'].to_csv('/Users/austinwhaley/Desktop/DSI-SF-4-austinmwhaley/other_datasets/nn_results.csv')\n",
    "#     #send_email(email_address, password, msg_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ARCHIVE: EXAMPLE OF NON-CLASS STYLE RNN\n",
    "# #INPUT\n",
    "# inputs = Input((None, X_train.shape[1]))\n",
    "\n",
    "# #DENSE_1\n",
    "# m = TimeDistributed(Dense(512, activation='relu'))(inputs)\n",
    "# m = Dropout(0.5)(m)\n",
    "# #DENSE_2\n",
    "# m = TimeDistributed(Dense(512, activation='relu'))(m)\n",
    "# m = Dropout(0.5)(m)\n",
    "# #DENSE_3\n",
    "# gru = (GRU(512, activation='relu'))(m)\n",
    "# gru = Dropout(0.5)(gru)\n",
    "# #DENSE_4\n",
    "# m = TimeDistributed(Dense(512, activation='relu'))(m)\n",
    "# m = Dropout(0.5)(m)\n",
    "# #DENSE_5\n",
    "# m = TimeDistributed(Dense(512, activation='relu'))(m)\n",
    "# m = Dropout(0.5)(m)\n",
    "\n",
    "# #OUTPUT\n",
    "# out = TimeDistributed(Dense(263, activation='relu'))(m)\n",
    "\n",
    "# #MODEL\n",
    "# model = Model(input=inputs, output=out)\n",
    "# model.compile(loss='mse', optimizer=Adamax(lr=0.001), metrics=[custom_r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ARCHIVE: NON-CLASS SAVE RESULTS\n",
    "# file_name = str(datetime.now()).replace(':', '_').replace(' ','_')[:19]\n",
    "# results.to_csv('/Users/austinwhaley/Desktop/DSI-SF-4-austinmwhaley/other_datasets/results_'+ file_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ARCHIVE: USED TO MAKE DAY_OF_WEEK COLUMN\n",
    "# ### FEB 1 was a SUNDAY\n",
    "# DAY_OF_WEEK = []\n",
    "# counter_start = 5\n",
    "# counter = counter_start\n",
    "# for i in range((len(uber)+1)/24):\n",
    "#     counter += 1\n",
    "#     if counter == 0:\n",
    "#         r = 'Monday'\n",
    "#     elif counter == 1:\n",
    "#         r = 'Tuesday'\n",
    "#     elif counter == 2:\n",
    "#         r = 'Wednesday'\n",
    "#     elif counter == 3:\n",
    "#         r = 'Thursday'\n",
    "#     elif counter == 4:\n",
    "#         r = 'Friday'\n",
    "#     elif counter == 5:\n",
    "#         r = 'Saturday'\n",
    "#     elif counter == 6:\n",
    "#         r = 'Sunday'\n",
    "#     else:\n",
    "#         r = 'ERROR'\n",
    "    \n",
    "#     if counter == 6:\n",
    "#         counter = -1\n",
    "    \n",
    "#     DAY_OF_WEEK.extend([r]*24)\n",
    "\n",
    "# uber.insert(4, 'DAY_OF_WEEK', DAY_OF_WEEK[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ARCHIVE: NON-CLASS LOAD WEIGHTS\n",
    "#model.load_weights('/Users/austinwhaley/Desktop/DSI-SF-4-austinmwhaley/other_datasets/uber3_attn_mse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ARCHIVE: EDA 1 of 3\n",
    "#103, 104, 199\n",
    "#Used this to insert np.zeros columns for missing locationIDs\n",
    "# zeros = np.zeros(shape=(3599,))\n",
    "# uber.insert(106, '103', zeros) #Worked\n",
    "# uber.insert(107, '104', zeros) #Worked\n",
    "# uber.insert(202, '199', zeros) #Worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###ARCHIVE: NON-CLASS SAVE_WEIGHTS\n",
    "# model.save_weights('/Users/austinwhaley/Desktop/DSI-SF-4-austinmwhaley/other_datasets/uber3_attn_mse.h5', overwrite=True)\n",
    "# print 'Saved ||', str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ARCHIVE: NON-CLASS PLOT RESULTS\n",
    "# results = pd.DataFrame(history.history)\n",
    "# #results = pd.read_csv('/Users/austinwhaley/Desktop/DSI-SF-4-austinmwhaley/other_datasets/uber_results_mse_attn_1000').drop('Unnamed: 0', 1)\n",
    "\n",
    "# # SUMMARY RESULTS\n",
    "# print '---MAX RESULTS--- \\n', results[['custom_r2','val_custom_r2']].iloc[:].max(), '\\n'\n",
    "# print '---TAIL RESULTS--- \\n', results[['custom_r2','val_custom_r2']].iloc[-1:], '\\n'\n",
    "\n",
    "# # Build figure\n",
    "# fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12,4.5))\n",
    "\n",
    "# # Assign Axis Plots\n",
    "# ax0.plot(results[['custom_r2', 'val_custom_r2']].iloc[:])\n",
    "# ax1.plot(results[['loss', 'val_loss']].iloc[:])\n",
    "\n",
    "# # Add horizontal and vertical line for max\n",
    "# ax0.axhline(results['val_custom_r2'].max(), color='red', lw=0.5)\n",
    "# ax0.axvline(results['val_custom_r2'].idxmax(), color='red', lw=0.5)\n",
    "\n",
    "# #Set axis limits\n",
    "# #ax0.set_ylim(bottom=0.5, top=0.6)\n",
    "\n",
    "# # Rename axis titles\n",
    "# ax0.set_title('r2', fontsize=15)\n",
    "# ax1.set_title('loss', fontsize=15)\n",
    "\n",
    "# ax0.legend(loc='lower right')\n",
    "\n",
    "# # Show plots\n",
    "# plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### ARCHIVE: EDA 2 of 3\n",
    "#uber['Wind SpeedMPH'].value_counts()\n",
    "#uber['Wind SpeedMPH'].count()\n",
    "#uber['Wind SpeedMPH'].unique()\n",
    "#uber['Wind SpeedMPH'] = pd.to_numeric(uber['Wind SpeedMPH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ARCHIVE: EDA 3 of 3\n",
    "#uber['Humidity'] = uber['Humidity'].map(lambda x: np.mean(uber['Humidity']) if pd.isnull(x) == True else x)\n",
    "#uber['Wind SpeedMPH'] = uber['Wind SpeedMPH'].map(lambda x: 0 if x == 'Calm' else x)\n",
    "#uber['Wind SpeedMPH'] = uber['Wind SpeedMPH'].map(lambda x: np.mean(uber['Wind SpeedMPH'] if x == -9999.0 else x))\n",
    "#print uber['Wind SpeedMPH'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def send_email(email_address, password, msg_, textfile):\n",
    "    \n",
    "    '''Send email with rsquared list & datetime'''\n",
    "    \n",
    "    import smtplib\n",
    "    from email.MIMEMultipart import MIMEMultipart\n",
    "    from email.MIMEText import MIMEText\n",
    "    \n",
    "    \n",
    "    \n",
    "    textfile = textfile\n",
    "    \n",
    "    server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    server.starttls()\n",
    "    server.login(email_address, password)\n",
    "    \n",
    "    # Open a plain text file for reading.  For this example, assume that\n",
    "    # the text file contains only ASCII characters.\n",
    "    fp = open(textfile, 'rb')\n",
    "    # Create a text/plain message\n",
    "    msg = MIMEText(fp.read())\n",
    "    fp.close()\n",
    "\n",
    "    msg['Subject'] = msg_ + ' - ' + str(datetime.now())\n",
    "    \n",
    "    server.sendmail(email_address, email_address, msg.as_string())\n",
    "    server.quit()\n",
    "    print 'Email Sent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### ARCHIVE: CLEANUP FOR VISUALIZATION\n",
    "# ### Manual Corrections\n",
    "# #  Arrochar (5)\n",
    "# zones.ix[5, 'Latitude'] = 40.594338\n",
    "# zones.ix[5, 'Longitude'] = -74.071329\n",
    "\n",
    "# # Eltingville(83)\n",
    "# zones.ix[83, 'Latitude'] = 40.545350\n",
    "# zones.ix[83, 'Longitude'] = -74.161806\n",
    "\n",
    "# # Penn Station (185)\n",
    "# zones.ix[185, 'Latitude'] = 40.750804\n",
    "# zones.ix[185, 'Longitude'] = -73.994098\n",
    "\n",
    "# # Ravenswood (192)\n",
    "# zones.ix[192, 'Latitude'] = 40.754391\n",
    "# zones.ix[192, 'Longitude'] = -73.944909\n",
    "\n",
    "# # Turtle Bay South (232)\n",
    "# zones.ix[232, 'Latitude'] = 40.756135\n",
    "# zones.ix[232, 'Longitude'] = -73.969346\n",
    "\n",
    "# # Whitestone (251)\n",
    "# zones.ix[251, 'Latitude'] = 40.785845\n",
    "# zones.ix[251, 'Longitude'] = -73.811042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ARCHIVE: USED TO GET LAT/LON FOR LOCATION_IDS\n",
    "# for i, zone in enumerate(zones.Zone):\n",
    "#     try:\n",
    "#         zone_name = zone.replace(' ', '')\n",
    "#         json = pd.read_json('https://maps.googleapis.com/maps/api/geocode/json?key=AIzaSyD3PRRjN1TXyhtE3M8nTf66NNWjGNrtIGA&new_forward_geocoder=true&address='+zone_name)\n",
    "#         zones.ix[i, 'Latitude'] = json.results[0]['geometry']['location']['lat']\n",
    "#         zones.ix[i, 'Longitude'] = json.results[0]['geometry']['location']['lng']\n",
    "#         print i, '\\t Success w/', zone_name\n",
    "#     except:\n",
    "#         print i, '\\t Error w/', zone_name\n",
    "#         zones.ix[i, 'Latitude'] = 'ERROR'\n",
    "#         zones.ix[i, 'Longitude'] = 'ERROR'\n",
    "# print 'COMPLETE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ARCHIVE: Attention Layer targets the last layer of the dense layers. \n",
    "### The attention weights are generated by the first GRU and fed\n",
    "### to the second GRU that is then finally filtered through one\n",
    "### last dense layer and then output\n",
    "\n",
    "# inputs = Input((None, X_train.shape[1]))\n",
    "\n",
    "# m = TimeDistributed(Dense(512, activation='relu'))(inputs)\n",
    "# m = Dropout(0.5)(m)\n",
    "# m = TimeDistributed(Dense(512, activation='relu'))(m)\n",
    "# m = Dropout(0.5)(m)\n",
    "# m = TimeDistributed(Dense(512, activation='relu'))(m)\n",
    "# m = Dropout(0.5)(m)\n",
    "\n",
    "# attn = GRU(512, return_sequences=True, stateful=False, consume_less='cpu')(m)\n",
    "# attn = Activation('softmax')(attn)\n",
    "\n",
    "# m_attn = merge([m, attn], mode='mul')\n",
    "\n",
    "# gru = GRU(512, return_sequences=True, stateful=False, consume_less='cpu')(m_attn)\n",
    "# gru = Dropout(0.5)(gru)\n",
    "\n",
    "# mix = TimeDistributed(Dense(512, activation='relu'))(gru)\n",
    "# mix = Dropout(0.5)(mix)\n",
    "\n",
    "# out = TimeDistributed(Dense(263, activation='relu'))(mix)\n",
    "# model = Model(input=inputs, output=out)\n",
    "# model.compile(loss='mse', optimizer=Adamax(lr=0.001), metrics=[custom_r2])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
